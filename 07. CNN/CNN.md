# 7. 합성곱 신경망 (CNN)

* 합성곱 신경망 (Convolutional Neural Network)
    - 이미지 인식과 음성 인식 등 다양한 곳에서 사용
    - 특히 이미지 인식 분야에서 딥러닝을 활용한 기법은 거의 CNN 을 기초로 함
    - CNN 메커니즘을 설명하고, 파이썬으로 구현함

## 7.1 전체 구조

* CNN 네트워크 구조
    - 지금까지의 신경망과 같이 레고 블럭 처럼 계층을 조합하여 만듦
    - 합성곱 계층 (convolutional layer) 과 풀링 계층 (pooling layer) 가 새롭게 등장함

* 완전 연결 신경망
    - 지금까지의 신경망 : 인접하는 계층의 모든 뉴런과 결합함
    - 완전 연결 (fully-connected) : Affine 계층이라는 이름으로 구현
    - 완전 연결 신경망 : Affine 계층 뒤에 ReLU 계층 (또는 Sigmoid 계층) 이 이어짐
    - 그림 7-1 : 5-층 신경망 - Affine-ReLU 조합 4개 쌓고, 마지막 층은 Affine-Softmax 임

* CNN 구조
    - '합성곱 계층 (Conv)' 과 '풀링 계층 (Pooling)' 추가
    - CNN 계층 : Conv-ReLU-(Pooling) 으로 연결 (풀링 계층은 생략 가능)
    - Affine-ReLU 가 Conv-ReLU-(Pooling) 으로 바뀐 형태
    - 그림 7-2 : 일반적인 5-층 CNN - 출력에 가까운 층은 Affine-ReLU, 마지막 층은 Affine-Softmax 그대로 사용

## 7.2 합성곱 계층

* CNN 과 완전 연결 신경망 차이점
    - '패딩 (padding)', '스트라이드 (보폭, stride)' 등 CNN 고유 용어 등장
    - 계층 간에 3차원 데이터 같은 입체적인 데이터가 흐름

* CNN 에서 사용하는 합성곱 계층의 구조를 살펴봄

### 7.2.1 완전 연결 계층의 문제점

* 완전 연결 계층
    - 완전 연결 신경망에서 사용
    - 인접하는 계층의 뉴런을 모두 연결하고 출력의 수는 임의로 정함

* 완전 연결 계층의 문제점
    - 데이터의 형상을 무시함
    - 이미지의 경우 세로, 가로, 채널로 구성된 3차원 데이터이지만, 완전 연결 계층에 입력할 때는 이를 평평한 1차원 데이터로 바꿔줌
    - MNIST 데이터셋 예제에서도 (채널 1, 가로 28, 세로 28) 인 이미지를 784개의 데이터로 Affine 계층에 입력함
    - 이미지는 3차원 형상이며, , 소중한 공간적 정보가 담겨 있음
    - 가까운 픽셀은 비슷하고, RGB 채널사이의 관계는 밀접하며, 먼 픽셀은 관련이 없는 등, 3차원 속에서 의미를 갖는 본질적인 패턴이 있을 것임
    - 완전 연결 계층은 형상을 무시하고 모든 입력 데이터를 동등한 (차원의) 뉴런으로 취급하여 형상에 담긴 정보를 살릴 수 없음

* 합성곱 계층
    - 형상을 유지함
    - 이미지도 3차원 데이터로 입력받으며, 다음 계층에도 3차원 데이터로 전달함
    - 그래서 CNN 은 이미지 처럼 형상을 가진 데이터를 제대로 이해할 (가능성이 있을) 수 있음

* 특징 맵 (feature map)
    - CNN 합성곱 계층의 입출력 데이터
    - 입력 특징 맵 (input feature map) : 합성곱 계층의 입력 데이터
    - 출력 특징 맵 (output feature map) : 합성곱 계층의 출력 데이터
    - 이 책에서는 '입출력 데이터' 와 '특징 맵' 을 같은 의미로 사용

### 7.2.2 합성곱 연산

* 합성곱 연산
    - 합성곱 계층에서는 합성곱 연산을 처리함
    - 이미지 처리에서 말하는 '필터 연산' 에 해당함
    - 그림 7-3 : 합성곱 연산은 입력 데이터에 필터를 적용함
    - 입력 데이터는 세로, 가로 방향의 형상을 가짐, 필터 역시 세로, 가로 방향의 차원을 가짐
    - 데이터와 필터의 형상은 (높이-height, 너비-width) 로 표기
    - 문헌에 따라 '필터' 를 '커널' 이라 하기도 함

* 합성곱 연산의 계산 순서
    - 그림 7-4
    - 합성곱 연산은 필터의 '윈도우 (window)' 를 일정 간격으로 이동하면서 입력 데이터에 적용함
    - 윈도우는 이미지에서 필터가 적용되는 부분-회색 부분-을 말함
    - 단일 곱셈-누산 (Fused Multiply-Add) : 입력과 필터에서 서로 대응하는 원소끼리 곱한 후 총합을 구함
    - 결과를 출력의 해당 장소에 저장
    - 모든 장소에서 수행하면 합성곱 연산의 출력 완성

* 가중치 매개 변수와 편향
    - 완전 연결 신경망에서는 가중치 매개 변수와 편향 존재
    - CNN 에서는 '필터의 매개 변수' 가 '가중치' 에 해당함
    - CNN 에도 편향 존재 : 그림 7-3 은 필터 적용 단계까지만 보인 것
    - 그림 7-5 : 편향까지 포함한 흐름 - 편향은 필터를 적용한 후의 데이터에 더함
    - 편향은 항상 하나 (1, 1) 만 존재 : 이 하나의 값을 필터를 적용한 모든 원소에 더함

### 7.2.3 패딩

* 패딩 (padding)
    - 합성곱 연산을 수행하기 전에 입력 데이터 주변을 (0 같은) '특정 값' 으로 채우는 것
    - 합성곱 연산에서 자주 사용하는 기법
    - 그림 7-6 : (4, 4) 크기의 입력 데이터에 폭이 1인 패팅을 적용한 모습
    - 처음에 크기가 (4, 4) 인 입력 데이터에 패딩을 추가하여 (6, 6) 이 됨
    - 이 입력에 (3, 3) 크기의 필터를 걸면 (4, 4) 크기의 출력 데이터를 생성함
    - 패딩은 1 외에도 2 나 3 등 원하는 정수로 설정할 수 있음
    - 그림 7-5 에 패딩을 2로 설정하면 입력 데이터의 크기는 (8, 8), 3으로 설정하면 (10, 10) 이 됨

> 패딩은 주로 출력 크기를 조정할 목적으로 사용함
> 예를 들어, (4, 4) 입력 데이터에 (3, 3) 필터를 적용하면 출력은 (2, 2) 가 되어, 입력보다 2만큼 줄어듦
> 합성곱 연산을 되풀이하는 심층 신경망에서는 문제가 될 수 있음
> 합성곱 연산을 거칠 때마다 크기가 작아져서 어느 시점에 출력 크기가 1이 되면 더 이상 합성곱 연산을 적용할 수 없게됨
> 이를 막기 위해 패딩 사용
> 패딩 폭을 1로 설명하면 (4, 4) 입력에 대한 출력이 (4, 4) 로 유지됨
> 입력 데이터의 크기를 고정한 채로 다음 계층에 전달할 수 있음

### 7.2.4 스트라이드

* 스트라이드 (stride)
    - 필터를 적용하는 위치의 간격
    - 스트라이드를 2로 하면 필터를 적용하는 윈도우가 두 칸씩 이동함
    - 그림 7-7 : 크기가 (7, 7) 인 입력 데이터에 스트라이드를 2로 설정한 필터 적용
    - 스트라이트를 2로 하니 출력이 (3, 3) 이 됨

* 출력 크기
    - 스트라이드를 키우면 출력 크기는 작아짐
    - 패딩을 크게 하면 출력 크기가 커짐
    - 입력 크기 : (H, W)
    - 필터 크기 : (FH, FW)
    - 출력 크기 : (OH, OW)
    - 패딩 : P
    - 스트라이드 : S
    - 식 7-1
    - <img src="https://latex.codecogs.com/svg.latex?OH&space;=&space;\frac{H&plus;2P-FH}{S}&space;&plus;&space;1" title="OH = \frac{H+2P-FH}{S} + 1" />
    - <img src="https://latex.codecogs.com/svg.latex?OW&space;=&space;\frac{W&plus;2P-FW}{S}&space;&plus;&space;1" title="OW = \frac{W+2P-FW}{S} + 1" />

* 예제
    - 입력 (4, 4), 패딩 1, 스트라이드 1, 필터 (3, 3) 
    - <img src="https://latex.codecogs.com/svg.latex?OH&space;=&space;\frac{4&plus;2\cdot&space;1-3}{1}&space;&plus;&space;1=4" title="OH = \frac{4+2\cdot 1-3}{1} + 1=4" />
    - <img src="https://latex.codecogs.com/svg.latex?OW&space;=&space;\frac{4&plus;2\cdot&space;1-3}{1}&space;&plus;&space;1=4" title="OW = \frac{4+2\cdot 1-3}{1} + 1=4" />
    - 입력 (7, 7), 패딩 0, 스트라이드 2, 필터 (3, 3) 
    - <img src="https://latex.codecogs.com/svg.latex?OH&space;=&space;\frac{7&plus;2\cdot&space;0-3}{2}&space;&plus;&space;1=3" title="OH = \frac{7+2\cdot 0-3}{2} + 1=3" />
    - <img src="https://latex.codecogs.com/svg.latex?OW&space;=&space;\frac{7&plus;2\cdot&space;0-3}{2}&space;&plus;&space;1=3" title="OH = \frac{7+2\cdot 0-3}{2} + 1=3" />
    - 입력 (28, 31), 패딩 2, 스트라이드 3, 필터 (5, 5) 
    - <img src="https://latex.codecogs.com/svg.latex?OH&space;=&space;\frac{28&plus;2\cdot&space;2-5}{3}&space;&plus;&space;1=10" title="OH = \frac{28+2\cdot 2-5}{3} + 1=10" />
    - <img src="https://latex.codecogs.com/svg.latex?OW&space;=&space;\frac{31&plus;2\cdot&space;2-5}{3}&space;&plus;&space;1=11" title="OH = \frac{31+2\cdot 2-5}{3} + 1=11" />
    - 식 7-1 에 값을 단순히 대입하면 출력 크기를 구할 수 있음
    - 단 식에서 분수식이 정수로 나눠 떨어지는 값이어야 함 : 출력 크기가 정수가 아니면 오류를 발생하는 등의 대응을 해줘야 함
    - 딥러닝 프레임웍 중에는 값이 나눠 떨어지지 않을 경우 가장 가까운 정수로 반올림 하는 등, 에러 없이 진행하도록 구현하는 경우도 있음

### 7.2.5 3차원 데이터의 합성곱 연산

* 3차원 데이터
    - 지금까지 2차원 형상을 다루는 합성곱 연산을 살펴봄
    - 이미지도 세로, 가로, 채널까지 3차원 데이터임
    - 채널까지 고려한 3차원 데이터를 다루는 합성곱 연산을 살펴봄

* 3차원 데이터의 합성곱 연산 예
    - 그림 7-8
    - 2차원일 때와 비교하여, 채널 방향으로 특징 맵이 늘어남

* 3차원 데이터의 합성곱 연산 순서
    - 그림 7-9
    - 채널 방향으로 특징 맵이 여러 개가 있으면 입력 데이터와 필터의 합성곱 연산을 채널마다 수행하고, 그 결과를 더해서 하나의 출력으로 얻음     

* 주의할 점
    - 3차원 합성곱 연산에서 입력 데이터 채널 수와 필터의 채널 수가 같아야 함
    - 그림 7-8, 7-9 에서는 모두 3개로 일치함
    - 필터 자체의 크기는 원하는 값으로 설정할 수 있음 : 단, 모든 채널의 필터는 같은 크기여야 함
    - 그림 7-8, 7-9 에서는 필터 크기가 (3, 3) 이지만, (2, 2) , (1, 1), 또는 (5, 5) 등도 가능함

### 7.2.6 블럭으로 생각하기

* 블럭
    - 3차원 합성곱 연산은 데이터와 필터를 직육면체 블럭으로 생각하면 쉬움
    - 그림 7-10 : 블럭 - 3차원 직육면체
    - 3차원 데이터를 다차원 배열로 나타낼 때는 (채널 channel, 높이 height, 너비 width) 순서로 씀
    - (C, H, W) : 채널 수 C, 높이 H, 너비 W 인 데이터 형상 
    - 필터도 같은 순서임
    - (C, FH, FW) : 채널 수 C, 필터 높이 H, 필터 너비 W

* 출력 데이터
    - '츨력 데이터' 는 한 장의 '특징 맵' : 채널이 1개인 특징 맵
    - (C, H, W) * (C, FH, FW) → (1, OH, OW)
    
* 필터의 개수
    - 합성곱 연산의 출력으로 다수의 채널을 내보내려면, '필터 (가중치)' 를 다수 사용하면 됨
    - 그림 7-11
    - 필터를 FN 개 사용하면, 출력 맵도 FN 개가 생성됨
    - FN 개의 맵을 모으면 형상이 (FN, OH, OW) 인 블럭이 완성됨
    - 이 완성된 블럭을 다음 계층으로 넘기겠다는 것이 'CNN 의 처리 흐름' 임

* 4차원 데이터
    - 합성곱 연산에서는 필터의 개수도 고려해야 함
    - 따라서 '필터의 가중치 데이터' 는 4차원 데이터이며 (출력 채널 수, 입력 채널 수, 높이, 너비) 순서로 씀
    - 채널 수 3, 크기 5x5 인 필터가 20개 면, (20, 3, 5, 5)
    - 사실 (필터 개수, 채널 개수, 높이, 너비) 가 아닐까?

* 편향 
    - 합성곱 연산에도 (완전 연결 계층 처럼) 편향을 씀
    - 그림 7-12 : 그림 7-11 에 편향을 더한 모습
    - 편향은 채널 하나에 값 하나씩으로 구성됨
    - 여기서는 편향 형상이 (FN, 1, 1) 이고, 필터의 출력 형상은 (FN, OH, OW) 임
    - 이 두 블럭을 더하면 편향의 각 값이 필터 출력인 (FN, OH, OW) 블럭의 대응 채널 모든 원소에 더해짐
    - 형상이 다른 블럭의 덧셈은 NumPy 의 브로드캐스트 기능으로 쉽게 구현 가능함 (1.5.5 브로드캐스트 참고)

### 7.2.7 배치 처리

* 신경망 처리의 배치
    - 신경망 처리에서는 입력 데이터를 한 덩어리로 묶어서 배치로 처리함
    - 완전 연결 신경망 구현에서는 이 방식을 지원하여 처리 효율을 높이고, 미니 배치 방식의 학습도 지원함

* 합성곱 연산의 배치
    - 합성곱 연산도 배치 처리를 지원하려고 함
    - 각 계층을 흐르는 데이터의 차원을 하나 늘려 4차원 데이터로 저장함
    - 데이터를 (데이터 수, 채널 수, 높이, 너비) 순으로 저장

* 합성곱 연산의 배치 처리 흐름
    - 그림 7-13 : 데이터가 N 개일 때, 그림 7-12 배치 처리 시의 데이터 흐름
    - 각 데이터의 선두에 배치용 차원을 추가함
    - 데이터는 4차원 형상을 가지고 각 계층을 타고 흐름
    - 신경망에 4차원 데이터가 하나 흐를 떄마다 데이터 N 개에 대한 합성곱 연산이 이뤄짐 : N 회분의 처리를 한 번에 수행

## 7.3 풀링 계층

* 풀링 (pooling)
    - 세로, 가로 방향의 공간을 줄이는 연산
    - 그림 7-14 : 2x2 영역을 원소 하나로 집약하여 공간 크기를 줄임, '2x2 최대 풀링' 을 스트라이드 2 로 처리하는 순서
    - 2x2 최대 풀링 (max pooling) : 2x2 영역에서 가장 큰 원소를 하나 꺼내는 것
    - 풀링의 윈도우 크기와 스트라이드는 같은 값으로 설정하는 것이 일반적임

> 평균 풀링 (average pooling) : 대상 영역의 평균을 계산함
>
> 이미지 인식 분야에서는 주로 최대 풀링을 사용함 : 이 책의 풀링 계층도 '최대 풀링' 을 의미함

### 7.3.1 풀링 계층의 특징

1. 학습해야 할 매개 변수가 없다
    - 풀링 계층은 합성곱 계층과 달리 학습해야 할 매개 변수가 없음
    - 풀링은 대상 영역에서 최대값 또는 평균을 취하는 명확한 처리이므로 특별히 학습할 것이 없음

2. 채널 수가 변하지 않는다
    - 풀링 연산은 입력 데이터의 채널 수 그대로 출력 데이터로 내보냄 : 채널마다 독립적으로 계산하기 때문
    - 그림 7-15

3. 입력의 변화에 영향을 적게 받는다 (강건함)
    - 입력 데이터가 조금 변해도 풀링의 결과는 잘 변하지 않음
    - 7-16 : (데이터를 오른쪽으로 1칸 이동하는) 입력 데이터의 차이를 풀링이 흡수하여 사라지게 하는 모습

## 7.4 합성곱/풀링 계층 구현하기

* 합성곱 계층과 풀링 계층을 파이썬으로 구현
    - 5장 오차 역전파법 처럼, 이번 절에서 구현하는 class 에도 `forward` 와 `backward` 메소드를 추가하여 모듈로 이용 가능하도록 함
    - 합성곱 계층과 풀링 계층을 '트릭' 을 사용하여 쉽게 구현하도록 함

### 7.4.1 4차원 배열

* CNN 에서 계층 사이를 흐르는 데이터는 4차원
* 데이터 형상이 (10, 1, 28, 28) 이면, 높이 28, 너비 28, 채널 1개인 데이터가 10개임
* 파이썬 구현 : jupytor

- 첫 번째 데이터에 접근 : `x[0]`
- 두 번째 데이터에 접근 : `x[1]`
- 파이썬 구현 : jupytor

* 첫 번째 데이터의 첫 채널 공간 데이터에 접근 
* 파이썬 구현 : jupytor

- CNN 이 4차원 데이터를 다뤄서 합성곱 연산 구현이 복잡할 것 같지만, `im2col` 이라는 '트릭' 이 문제를 단순하게 만듦

### 7.4.2 im2col 로 데이터 전개하기

* 합성곱 연산의 구현
    - 합성곱 연산을 그냥 구현하려면 `for` 문을 겹겹이 써야함
    - 귀찮고, 성능이 떨어지는 단점이 있음 : NumPy 에서는 원소 접근시 for 문을 사용하지 않는 것이 바람직
    - for 문 대신 `im2col` 이라는 편의 함수로 구현함

* im2col
    - 입력 데이터를 필터링 하기 좋게 전개하는 (가중치 계산을 하기 좋게 펼치는) 함수
    - 그림 7-17 : 3차원 입력 데이터에 im2col 을 적용하면 2차원 행렬로 바뀜 - 사실 배치 개수까지 포함한 4차원 데이터를 2차원으로 변환함
    - im2col 은 필터링하기 좋게 입력 데이터를 전개함
    - 그림 7-18 : 입력 데이터에서 필터를 적용하는 영역 (3차원 블럭) 을 한 줄로 늘어 놓음
    - 필터를 적용하는 모든 영역에 대해 이러한 전개를 수행하는 것이 `im2col` 임

* im2col 의 장단점
    - 그림 7-18 에서는 스트라이드를 크게 잡았지만, 실제 상황에서는 필터 적용 영역이 겹치는 경우가 대부분임
    - 필터 적용 영역이 겹치면 im2col 로 전개한 후의 원소 개수가 원래 블럭 원소 개수보다 많아짐
    - 그래서 im2col 을 사용하여 구현하면 평소보다 메모리를 많이 소비하는 단점이 있음
    - 하지만, 컴퓨터는 큰 행렬을 묶어서 계산하는데 탁월함
    - 행렬 계산 (선형 대수) 라이브러리는 행렬 계산에 고도로 최적화됨 : 큰 행렬 곱셈을 빠르게 계산함
    - 문제를 행렬 계산으로 만들면 선형 대수 라이브러리를 활용하여 효율을 높일 수 있음

> `im2col` 은 'image to column (이미지에서 행렬로)' 라는 의미임
>
> '카페 (caffe)', '체이너 (chainer)' 등의 딥러닝 프레임웍은 `im2col` 이라는 이름의 함수를 만들어 합성곱 계층 구현에 사용함

* 합성곱 계층의 구현 흐름
    - `im2col` 로 입력 데이터를 전개하면, 합성곱 계층의 필터를 '1열' 로 전개하여, 두 행렬의 내적을 계산함
    - 이는 완전 연결 계층의 Affine 계층에서 한 것과 거의 같음
    - 그림 7-19 : im2col 방식으로 출력한 결과는 2차원 행렬임
    - CNN 은 데이터를 4차원 배열로 저장하므로, 2차원 출력 데이터를 4차원으로 변형함

### 7.4.3 합성곱 계층 구현하기

* `im2col`
    - 이 책에서는 `im2col` 함수를 미리 만들어 제공함
    - 구현은 간단한 함수 10개 정도를 묶은 것
    - `common/util.py` 참고
    - 인터페이스 : `im2col(input_data, filter_h, filter_w, stride=1, pad=0)`
    - input_data : (데이터 개수, 채널 개수, 높이, 너비) 라는 4차원 배열로 이루어진 입력 데이터
    - im2col 은 필터 크기, 스트라이드, 패딩을 고려하여 입력 데이터를 2차원 배열로 전개함

* `im2col` 사용
    - jupytor 
    - 첫 번째는 배치 (데이터) 크기 1, 채널 3, 7x7 데이터
    - 두 번째는 배치 크기만 10 이고, 나머지는 첫 번째와 같음
    - im2col 함수 적용한 두 경우 모두 차원의 두 번째 원소는 75임 : 필터의 원소 개수 = (채널 3, 5x5 데이터)
    - 배치 크기가 1 이면, 결과 크기가 (9, 75) 이고, 10이면, 10배인 (90, 75) 인 데이터가 저장됨
    - (OH, OW) 가 (3, 3) 인 것을 1차원으로 만들었기 때문에 9 가 됨

* `im2col` 을 사용한 합성곱 계층 구현
    - Convolution 이라는 클래스로 구현
    - jupytor

```python
class Convolution:
    def __init__(self, W, b, stride=1, pad=0):
        self.W = W
        self.b = b
        self.stride = stride
        self.pad = pad

    def forward(self, x):
        FN, C, FH, FW = self.W.shape
        N, C, H, W = x.shape
        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)
        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)

        col = im2col(x, FH, FW, self.stride, self.pad)
        col_W = self.W.reshape(FN, -1).T

        out = np.dot(col, col_W) + self.b
        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)

        return out
```

* 합성곱 계층
    - 필터 (가중치), 편향, 스트라이드, 패딩을 인수로 받아 초기화
    - 필터는 (FN, C, FH, FW) 라는 4차원 형상
    - im2col 로 입력 데이터를 전개함
    - reshape 으로 필터도 2차원 배열로 전개함
    - 필터 전개 시에는 각 필터 블럭을 1줄로 펼쳐 세움
    - reshape 의 두 번째 인수인 `-1` 은 다차원 배열의 원소 수가 변환 후에도 똑같이 유지되도록 적저히 묶어주는 편의 기능임
    - 예를 들어, (10, 3, 5, 5) 형상의 다차원 배열 (750 개 원소) 에 `reshape(10, -1)` 를 호출하면 형상이 (10, 75) 인 배열 만듦 (750개 10묶음)
    - 전개한 두 행렬의 내적을 구함
    - forward 구현 마지막에서는 출력 데이터를 적절한 형상으로 바꿔줌
    - transpose 함수 : 다차원 배열의 축 순서를 바꿔주는 함수
    - (0으로 시작하는) 인덱스를 사용하여 축의 순서를 변경함
    - (N 0, H, 1, W 2, C 3) -> (N 0, C 3, H 1, W 2)
    - im2col 전개 덕부에 Affine 계층과 거의 똑같이 구현 가능함
    - 합성곱 계층의 역전파는 Affine 계층 구현과 공통점이 많아서 설명하지 않음
    - 합성곱 계층의 역전파는 im2col 을 역으로 처리해야 함 : com2im 함수 사용 - 구현은 common/util.py 에 있음
    - 합성곱 계층의 전체 구현은 common/layer.py 에 있음

### 7.4.4 풀링 계층 구현하기

* im2col
    - 풀링 계층의 구현도 합성곱 계층 처럼 im2col 을 사용하여 전개함
    - 단, 풀링은 채널 쪽이 독립적임
    - 그림 7-21 : 풀링은 적용 영역을 채널마다 독립적으로 전개함
    - 전개한 후에 행렬에서 행별 최대값을 구하고 적저한 형상으로 만들면 됨
    - 이것이 풀링 계층의 forward 처리 흐름임

* 풀링 계층의 구현
    - 코드 : jupytor
    - 세 단계로 진행
        1. 입력 데이터를 전개함
        2. 행별 최대값을 구함
        3. 적절한 모양으로 만듦
    - 각 단계는 한두 줄 정도로 간단하게 구현할 수 있음

```python
class Pooling:
    def __init__(self, pool_h, pool_w, stride=2, pad=0):
        self.pool_h = pool_h
        self.pool_w = pool_w
        self.stride = stride
        self.pad = pad
        
    def forward(self, x):
        N, C, H, W = x.shape
        out_h = int(1 + (H - self.pool_h) / self.stride)
        out_w = int(1 + (W - self.pool_w) / self.stride)

        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)
        col = col.reshape(-1, self.pool_h*self.pool_w)

        out = np.max(col, axis=1)
        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)

        return out
```

> 최대값 계산은 NumPy 의 np.max 메소드 사용 가능. 
>
> np.max 는 인수로 '축 (axis)' 을 지정할 수 있으며, 인수로 지정한 축마다 최대값을 구할 수 있음
>
> np.max(x, axis=1) 은 입력 x 의 1 번째 차원의 축마다 최대값을 구함

* 풀링 계층
    - 위 코드가 forward 처리
    - 입력 데이터를 풀링하기 쉬운 형태로 전개하면 그 이후의 구현이 간단해짐
    - backward 처리는 설명 생략 : "5.5.1 ReLU 계층" 구현 때 사용한 max 역전파 참고
    - 풀링 계층의 전체 구현은 common/layer.py 에 있음

## 7.5 CNN 구현하기

* SimpleConvNet
    - 손글씨 숫자를 인식하는 CNN : 구현한 합성곱 계층과 풀링 계층을 조합
    - 그림 7-23 : Conv-ReLU-Pooling-Affine-ReLU-Affine-Softmax 으로 구성한 3-층 CNN
    - 초기화 인수
        * input_dim : 입력 데이터 (C, H, W) 차원
        * conv_param : 합성곱 계층의 하이퍼 파라미터 (딕셔너리), 딕셔너리 키는 다음과 같음
            - filter_num : 필터 개수
            - filter_size : 필터 크기
            - stride : 스트라이드
            - pad : 패딩
        * hidden_size : 은닉층 (완전 연결) 뉴런 개수
        * output_size : 출력층 (완전 연결) 뉴런 개수
        * weight_init_std : 초기화 때의 가중치 표준 편차
    - 합성곱 계층의 하이퍼 파라미터는 딕셔너리로 주어짐 : ['filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1]

* SimpleConvNet 의 초기화 부부 설명
    - 세 부분으로 나누어서 설명

```python
class SimpleConvNet:
    def __init__(self, input_dim=(1, 28, 28), 
                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},
                 hidden_size=100, output_size=10, weight_init_std=0.01):
        filter_num = conv_param['filter_num']
        filter_size = conv_param['filter_size']
        filter_pad = conv_param['pad']
        filter_stride = conv_param['stride']
        input_size = input_dim[1]
        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1
        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))
```

    - 초기화 인자로 받은 합성곱 계층의 하이퍼 파라미터를 딕셔너리에서 꺼냄
    - 합성곱 계층의 출력 크기를 계산함

```python
        # 重みの初期化
        self.params = {}
        self.params['W1'] = weight_init_std * \
                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)
        self.params['b1'] = np.zeros(filter_num)
        self.params['W2'] = weight_init_std * \
                            np.random.randn(pool_output_size, hidden_size)
        self.params['b2'] = np.zeros(hidden_size)
        self.params['W3'] = weight_init_std * \
                            np.random.randn(hidden_size, output_size)
        self.params['b3'] = np.zeros(output_size)
```

    - 가중치 매개 변수를 초기화
    - 학습에 필요한 매개 변수는 1 번째 층의 합성곱 계층 및 나머지 두 완전 연결 계층의 가중치와 편향임
    - 이 매개 변수들을 인스턴스 변수 params 딕셔너리에 저장함

```python
        self.layers = OrderedDict()
        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],
                                           conv_param['stride'], conv_param['pad'])
        self.layers['Relu1'] = Relu()
        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)
        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])
        self.layers['Relu2'] = Relu()
        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])

        self.last_layer = SoftmaxWithLoss()
```

    - CNN 을 구성하는 계층을 생성함
    - 순서 있는 딕셔너리인 layers 에 계층들을 차례로 추가함
    - 마지막 SoftmaxWithLoss 계층은 lastLayer 라는 별도 변수에 저장함

* SimpleConvNet 나머지 부분
    - 추론을 수행하는 predict 메소드와 손실 함수의 값을 구하는 loss 메소드
        * 인수 x 는 입력 데이터, t 는 정답 레이블
        * 추론을 수행하는 predict 메소드는 초기화 때 layers 에 추가한 계층을 맨 앞에서부터 차례로 forward 메소드를 호출하며 결과를 다음 계층에 전달
        * 손실 함수를 구하는 loss 메소드는 predict 메소드 결과를 인자로 마지막 층의 forward 메소드를 호출함
        * 즉, 첫 계층부터 마지막 계층까지 forward 처리를 함

```python
    def predict(self, x):
        for layer in self.layers.values():
            x = layer.forward(x)

        return x

    def loss(self, x, t):
        y = self.predict(x)
        return self.last_layer.forward(y, t)
```

    - 오차 역전파법으로 기울기를 구하는 구현
        * 매개 변수의 기울기는 오차 역전파법으로 구함 : 순전파와 역전파를 반복함
        * 지금까지 순전파와 역전파를 제대로 구현했으면, 여기서는 적절한 순서로 이들을 호출해주면 됨
        * 마지막으로 grad 라는 딕셔너리 변수에 각 가중치 매개 변수의 기울기를 저장함

```python
    def gradient(self, x, t):
        # forward
        self.loss(x, t)

        # backward
        dout = 1
        dout = self.last_layer.backward(dout)

        layers = list(self.layers.values())
        layers.reverse()
        for layer in layers:
            dout = layer.backward(dout)

        # 設定
        grads = {}
        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db
        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db
        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db

        return grads
```

    - 여기 까지가 SimpleConvNet 구현임

* SimpleConvNet 으로 MNIST 데이터셋 학습
    - 학습을 위한 코드 및 설명은 "4.5 학습 알고리즘 구현하기" 와 비슷하여 생략함
    - 소스 코드는 ch07/train_convnet.py 에 있음
    - SimpleConvNet 을 MNIST 데이터셋으로 학습하면 훈련 데이터에 대한 정확도는 99.92%, 시험 데이터에 대한 정확도는 98.96% 임
    - 시험 데이터에 대한 정확도가 99% 임은 비교적 작은 네트워크로서는 아주 높은 것임
    - 다음 장에서는 계층을 더 깊게 하여 시험 데이터에 대한 정확도가 99% 를 넘는 네트워크를 구현할 것임

* SimpleConvNet 결론
    - 합성곱 계층과 풀링 계층은 이미지 인식에 필수적인 모듈임
    - 이미지라는 공간적인 형삭에 담긴 특징을 CNN 이 잘 파악하여 손글씨 숫자 인식에서 높은 정확도를 달성할 수 있었음

## CNN 시각화하기

* CNN 을 구성하는 합성곱 계층은 입력 이미지 데이터에서 무엇을 보고 있는가?
* 이번 절은 합성곱 계층을 시각화해서 CNN 이 보고 있는 것이 무엇인지를 알아봄

### 7.6.1 첫 번째 층의 가중치 시각화하기

* MNIST 데이터셋의 CNN 학습
    - 첫 번째 층의 합성곱 계층 가중치 형상은 (30, 1, 5, 5) : (필터 30개, 채널 1개, 5x5 크기)
    - 필터 크기 5x5, 채널 1개 : 이 필터를 회색 이미지로 시각화 할 수 있다는 의미
    - 합성곱 (첫 번째) 계층의 필터를 이미지로 나타내 봄
    - 학습 전과 후의 가중치 비교 : 코드는 ch07/visualize_filter.py 에 있음

* 그림 7-24
    - 학습 전과 후의 첫 번째 층 가중치
    - 가중치 원소는 실수이나 이미지에서는 가장 작은 값 (0) 은 검은색, 가장 큰 값 (255) 는 흰색으로 정규화하여 표시함
    - 학습 전 필터는 무작위로 초기화하여 흑백 정도에 규칙성이 없음
    - 학습 후 필터는 규칙성 있는 이미지가 됨 : 흰색에서 검은색으로 점차 변화하고, '덩어리 (blob)' 가 있는 등의, 규칙을 띄는 필터로 바뀜
    - 규칙성 있는 필터가 보고 있는 것 : '경계선 (edge)' 와 '덩어리 영역 (blob)' 등을 보고 있음
    - 그림 7-25 : 학습된 필터 2개를 선택하여 입력 이미지에 합성곱 처리를 한 결과
    - 왼쪽 절반이 흰색이고 오른쪽 절반이 검은색인 필터는 세로 방향의 에지에 반응하는 필터임
    - '필터 1' 은 세로 에지에 반응하며 '필터 2' 는 가로 에지에 반응하는 것을 볼 수 있음

* 합성곱 계층의 필터
    - '경계선 (edge)' 이나 '덩어리 (blob)' 등의 원시적인 정보를 추출할 수 있음
    - 원시적인 정보를 뒤의 계층에 전달하는 것이 CNN 에서 일어나는 일임

### 7.6.2 층 깊이에 따른 추출 정보 변화

* CNN 의 각 계층에서 추출되는 정보
    - 첫 번째 층 : '경계선 (edge)' 이나 '덩어리 (blob)' 등의 저수준 정보를 추출
    - 계층이 깊어질 수록 추출되는 정보가 (강하게 반응하는 뉴런이) 더 추상화 됨 : 딥러닝 시각화에 대한 연구 (책 주석) 참고

* 그림 7-26
    - AlexNet : 일반 사물 인식을 수행한 8-층의 CNN - 합성곱 계층과 풀링 계층을 여러 겹 쌓고 마지막에 완전 연결 계층을 거쳐 출력하는 구조
    - 1 번째 층은 에지와 블롭, 3 번째 층은 텍스처, 5 번째 층은 사물의 일부, 마지막 완전 연결 계층은 사물의 클래스 (개, 자동차 등) 에 뉴런이 반응함

* 딥러닝
    - 딥러닝은 합성곱 계층을 여러 겹 쌓으면, 층이 깊어지면서 더 복잡하고 추상화된 정보 추출됨
    - 처음 층은 단순한 에지에 반응하고, 이어서 텍스처, 그리고 더 복잡한 사물 일부에 반응하도록 변화함
    - 층이 깊어지면서 뉴런이 반응하는 대상이 단순한 모양에서 고급 정보로 변화함 : 즉, 사물의 의미를 이해하도록 변화하는 것

## 7.7 대표적인 CNN

* CNN 의 종류
    - 현재까지 제안된 CNN 네트워크는 다양함
    - 특히 중요한 네트워크 두 가지 : LeNet - CNN 원조, AlexNet - 딥러닝이 주목받도록 한 네트워크

### 7.7.1 LeNet

* LeNet
    - 손글씨 숫자를 인식하는 네트워크 
    - 1998 년에 제안됨
    - 그림 7-27 : 합성곱 계층과 (서브 샘플링만 하는) 풀링 계층을 반복하고, 마지막에 완전 연결 계층을 거치면서 결과 출력
    - 현재의 CNN 과의 차이
        1. LeNet 은 시그모이드 함수를 활성화 함수로 사용, 현재 CNN 은 ReLU
        2. LeNet 은 서브 샘플링을 하여 중간 데이터의 크기가 작아지지만, 현재 CNN 은 최대 풀링이 주류임
    - LeNet 이 20년 전에 제안된 첫 CNN 임을 생각하면 차이는 크지 않음

### 7.7.2 AlexNet

* AlexNet
    - 비교적 최근인 2012 년에 발표, 딥러닝 열풍을 일으킴
    - 그림 7-28 : 구성은 LeNet 과 크게 다르지 않음 - 합성곱 계층과 풀링 계층을 거듭하며 마지막으로 완전 연결 계층을 거쳐 결과 출력
    - AlexNet 의 변화점
        1. 활성화 함수로 ReLU 사용
        2. LRN (Local Response Normalization) 이라는 국소적 정규화 계층을 이용함
        3. '드랍 아웃 (Drop Out)' 사용
    - 네트워크 구성면에서 LeNet 과 AlexNet 은 큰 차이가 없음 : 주변 환경과 컴퓨터 기술이 큰 진보를 이룬 것임
    - 빅 데이터와 GPU : 딥러닝 발전의 원동력

> 딥러닝 (심층 신경망) 에는 수많은 매개 변수가 사용됨 : 학습에는 엄청난 양의 계산, 매개 변수 피팅에 데이터도 대량으로 필요
>
> GPU 와 빅 데이터는 이런 문제에 해결책이 됨

## 7.8 정리

* CNN 에 대해 알아봄
* CNN 의 기본 모듈인 합성곱 계층과 풀링 계층은 다소 복잡하지만, 한 번 이해하면 사용하기만 하면 됨

* 이번 장에서 배운 것
    1. CNN 은 지금까지의 완전 연결 계층 네트워크에 합성곱 계층과 풀링 계층을 새로 추가함
    2. 합성곱 계층과 풀링 계층은 (이미지를 행렬로 전개하는 함수인) im2col 를 이용하면 간단하고 효율적으로 구현할 수 있음
    3. CNN 을 시각화해보면 계층이 깊어질 수록 고급 정보가 추출되는 모습을 확인 가능함
    4. 대표적인 CNN 에는 LeNet 과 AlexNet 이 있음
    5. 딥러닝 발전에는 빅 데이터와 GPU 가 크게 기여함