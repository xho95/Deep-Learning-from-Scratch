# 6. 학습 관련 기술들

* 신경망 학습에서 중요한 주제들
    1. 가중치 매개 변수의 최적값을 탐색하는 최적화 방법
    2. 가중치 매개 변수 초기값
    3. 하이퍼 파라미터 설정 방법
    4. 오버피팅 대응책 : 정규화 방법 - 가중치 감소, 드롭아웃
    5. 배치 정규화
* 이 장의 기법을 이용하면 신경망 학습의 효율과 정확도를 높일 수 있음

## 6.1 매개 변수 갱신

* 신경망 학습의 목적 : 손실 함수의 값을 가능한 낮추는 매개 변수를 찾는 것 - 최적 매개 변수를 찾는 문제
* 최적화 (Optimization) : 최적 매개 변수를 찾는 문제를 푸는 것
* 신경망 최적화는 아주 어려운 문제

- 확률적 경사 하강법 (SGD)
    - 매개 변수의 기울기인 미분을 이용하여 최적 매개 변수의 값을 찾음
    - 매개 변수의 기울기를 구하고, 기울어진 방향으로 매개 변수 값을 갱신하는 것을 반복하여 최적 매개 변수로 다가감
    - 매개 변수를 무작정 찾는 것보다는 똑똑한 방법임

* SGD 의 단점을 알아보고 다른 최적화 기법을 소개함

### 6.1.1 모험가 이야기

* 최적 매개 변수를 탐색하는 일은 지도도 없이 눈을 가리고 가장 낮은 골짜기를 찾는 것과 같음

### 6.1.2 확률적 경사 하강법 (SGD)

* SGD 복습

- [식 5-4] <img src="https://latex.codecogs.com/svg.latex?W\leftarrow&space;W-\eta&space;\frac{\partial&space;L}{\partial&space;W}" title="W\leftarrow W-\eta \frac{\partial L}{\partial W}" />
- <img src="https://latex.codecogs.com/svg.latex?\frac{\partial&space;L}{\partial&space;W}" title="{\partial L}{\partial W}" /> : W 에 대한 손실 함수의 기울기
- <img src="https://latex.codecogs.com/svg.latex?\eta" title="\eta" /> : 학습률 - 실제로는 0.01 이나 0.001 과 같은 값을 미리 정해서 사용함

* SGD 는 기울어진 방향으로 일정 거리만 가겠다는 단순한 방법

### 6.1.3 SGD 의 단점

### 6.1.4 모멘텀

### 6.1.5 AdaGrad

### 6.1.6 Adam

### 6.1.7 어느 갱신 방법을 이용할 것인가?

### 6.1.8 MNIST 데이터셋으로 본 갱신 방법 비교

## 6.2 가중치의 초기값

### 6.2.1 초기값을 0으로 하면?

### 6.2.2 은닉층의 활성화값 분포

### 6.2.3 ReLU 를 사용할 때의 가중치 초기값

### 6.2.4 MNIST 데이터셋으로 본 가중치 초기값 비교

## 6.3 배치 정규화

### 6.3.1 배치 정규화 알고리즘

### 6.3.2 배치 정규화의 효과

## 6.4 바른 학습을 위해

### 6.4.1 오버피팅

### 6.4.2 가중치 감소

### 6.4.3 드롭아웃

## 6.5 적절한 하이퍼 파라미터 값 찾기

### 6.5.1 검증 데이터

### 6.5.2 하이퍼 파라미터 최적화

### 6.5.3 하이퍼 파라미터 최적화 구현하기

## 6.6 정리

